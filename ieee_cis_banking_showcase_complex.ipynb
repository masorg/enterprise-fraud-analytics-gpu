{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8969d0",
   "metadata": {},
   "source": [
    "# Banking Fraud Analytics Showcase (IEEE‑CIS)  \n",
    "\n",
    "### Agenda\n",
    "1. **Business framing**: what “fraud detection” means operationally (alerts, false positives, costs).\n",
    "2. **Data understanding**: what’s in the dataset, how transactions relate to identity signals.\n",
    "3. **Data quality checks**: duplicates, missingness, cardinality, outliers, leakage watchlist.\n",
    "4. **EDA (on a safe sample)**: patterns by amount, time-of-day, device, email domain, etc.\n",
    "5. **Feature design**: practical features banks actually use (time buckets, velocity proxies, frequency encodings).\n",
    "6. **Modeling**:\n",
    "   - Baseline model (fast sanity check)\n",
    "   - Neural model (GPU-friendly) on engineered tabular features\n",
    "7. **Decisioning**: threshold by **alert rate** and by **cost**.\n",
    "8. **Benchmark**: demonstrate **CPU vs GPU** training time (epochs=2) using CUDA if available.\n",
    "9. **What “production readiness” looks like**: monitoring, drift, governance, privacy constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151fda9b",
   "metadata": {},
   "source": [
    "## 0) Setup and environment checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abbf823f-7d8b-47bb-bbc0-b4b063bea67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install CUDA-enabled PyTorch (explicit)\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# import torch\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Step 2: Install everything else\n",
    "# %pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b56768",
   "metadata": {},
   "source": [
    "### Environment Validation and Dependency Management\n",
    "\n",
    "This cell performs a comprehensive environment check to ensure all required dependencies are available. It verifies:\n",
    "- **Core libraries**: NumPy, Pandas, Scikit-learn for data manipulation and modeling\n",
    "- **Visualization**: Matplotlib and Seaborn for exploratory data analysis\n",
    "- **Data processing**: Polars for high-performance streaming operations\n",
    "\n",
    "We suppress warnings to keep the output clean during production runs. If any packages are missing, uncomment the pip install commands in the previous cell to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51781b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed (uncomment):\n",
    "# %pip -q install -U numpy pandas scipy scikit-learn matplotlib seaborn polars pyarrow psutil\n",
    "# %pip -q install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "import os, sys, time, subprocess, platform, pathlib, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ffc52",
   "metadata": {},
   "source": [
    "### System Resource Assessment\n",
    "\n",
    "This diagnostic cell performs a thorough system resource assessment to understand the compute environment:\n",
    "\n",
    "**Memory Detection:**\n",
    "- Checks both host-level RAM (via psutil) and container-level limits (via cgroups)\n",
    "- Critical for preventing out-of-memory crashes in containerized environments\n",
    "- Helps determine appropriate batch sizes and streaming strategies\n",
    "\n",
    "**GPU Availability:**\n",
    "- Detects NVIDIA GPUs using nvidia-smi\n",
    "- Essential for determining whether GPU-accelerated training is possible\n",
    "- Informs decisions about PyTorch device placement (CPU vs CUDA)\n",
    "\n",
    "**Shared Memory (shm):**\n",
    "- Checks /dev/shm capacity for DataLoader workers\n",
    "- Low shm can cause PyTorch multiprocessing errors\n",
    "- We'll configure num_workers=0 if shm is limited\n",
    "\n",
    "This information guides our downstream choices for data loading, batch sizing, and parallelization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5cf5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.13\n",
      "Platform: macOS-15.7.3-arm64-arm-64bit\n",
      "CPU cores: 11\n",
      "Host RAM total (GB): 36.0\n",
      "Host RAM avail (GB): 13.65\n",
      "cgroup memory cap (GB): not detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: nvidia-smi: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nvidia-smi detected (or no GPU visible).\n",
      "GPU_VISIBLE: False\n",
      "/dev/shm: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /dev/shm: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def bytes_to_gb(b): \n",
    "    return b / (1024**3)\n",
    "\n",
    "def read_cgroup_limit_bytes():\n",
    "    p = \"/sys/fs/cgroup/memory.max\"\n",
    "    if pathlib.Path(p).exists():\n",
    "        v = open(p).read().strip()\n",
    "        if v.isdigit():\n",
    "            return int(v)\n",
    "    p = \"/sys/fs/cgroup/memory/memory.limit_in_bytes\"\n",
    "    if pathlib.Path(p).exists():\n",
    "        v = open(p).read().strip()\n",
    "        if v.isdigit():\n",
    "            return int(v)\n",
    "    return None\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"CPU cores:\", os.cpu_count())\n",
    "print(\"Host RAM total (GB):\", round(bytes_to_gb(psutil.virtual_memory().total), 2))\n",
    "print(\"Host RAM avail (GB):\", round(bytes_to_gb(psutil.virtual_memory().available), 2))\n",
    "\n",
    "CGROUP_LIMIT = read_cgroup_limit_bytes()\n",
    "print(\"cgroup memory cap (GB):\", round(bytes_to_gb(CGROUP_LIMIT), 2) if CGROUP_LIMIT else \"not detected\")\n",
    "\n",
    "try:\n",
    "    print(subprocess.check_output([\"bash\",\"-lc\",\"nvidia-smi -L\"], text=True))\n",
    "    GPU_VISIBLE = True\n",
    "except Exception:\n",
    "    print(\"No nvidia-smi detected (or no GPU visible).\")\n",
    "    GPU_VISIBLE = False\n",
    "\n",
    "print(\"GPU_VISIBLE:\", GPU_VISIBLE)\n",
    "\n",
    "try:\n",
    "    shm = subprocess.check_output([\"bash\",\"-lc\",\"df -h /dev/shm | tail -n 1\"], text=True).strip()\n",
    "    print(\"/dev/shm:\", shm)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab0d9c-3076-4fe5-b676-6f9039f454f9",
   "metadata": {},
   "source": [
    "## 0.1) Extract Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbb766",
   "metadata": {},
   "source": [
    "### Data Extraction from Archive\n",
    "\n",
    "This cell extracts the IEEE-CIS fraud detection dataset from a compressed archive:\n",
    "\n",
    "**Dataset Source:**\n",
    "- Kaggle competition: [IEEE-CIS Fraud Detection](https://www.kaggle.com/competitions/ieee-fraud-detection)\n",
    "- Contains real-world transaction data with identity information\n",
    "\n",
    "**Files Expected:**\n",
    "- `train_transaction.csv`: Transaction-level features (amount, time, card info, etc.)\n",
    "- `train_identity.csv`: Identity-level features (device, browser, address, etc.)\n",
    "\n",
    "The extraction creates a `data/` directory structure that subsequent cells will reference. This one-time operation prepares the raw data for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca02788-8ea6-4a0a-9de8-aa70b84c5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "\n",
    "# Data source: https://www.kaggle.com/competitions/ieee-fraud-detection\n",
    "\n",
    "os.makedirs(\"data/data\", exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(\"data.zip\", \"r\") as z:\n",
    "    z.extractall(\"data\")\n",
    "\n",
    "print(\"Extracted files:\", os.listdir(\"data\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc599ff",
   "metadata": {},
   "source": [
    "## 1) Business framing: what we’re optimizing for\n",
    "\n",
    "Fraud modeling in banks usually becomes an **operations optimization** problem:\n",
    "\n",
    "- **False Positive (FP)** = customer friction + analyst time + manual review cost  \n",
    "- **False Negative (FN)** = fraud loss + chargebacks + regulatory/audit concerns\n",
    "\n",
    "That means:\n",
    "- AUC is useful, but **alert-rate decisions** (top X% flagged) often matter more.\n",
    "- We typically present results at **fixed alert rates** (e.g., 0.5%, 1%, 2%).\n",
    "- We may also present a **cost curve**: cost(FN) vs cost(FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609354c",
   "metadata": {},
   "source": [
    "### Data File Validation\n",
    "\n",
    "Performs basic validation to ensure the required CSV files exist at the expected paths:\n",
    "\n",
    "- Confirms presence of transaction data (`train_transaction.csv`)\n",
    "- Confirms presence of identity data (`train_identity.csv`)\n",
    "- Raises clear errors if files are missing, preventing silent failures downstream\n",
    "\n",
    "This is a defensive programming practice that catches data pipeline issues early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef31501f",
   "metadata": {},
   "source": [
    "## 2) Data access: robust join (streaming) to avoid RAM crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db0bc0",
   "metadata": {},
   "source": [
    "### Memory-Efficient Data Join Using Polars Streaming\n",
    "\n",
    "This cell performs a **left join** of transaction and identity data using Polars' streaming engine:\n",
    "\n",
    "**Why Polars Streaming?**\n",
    "- Processes data in chunks rather than loading everything into RAM\n",
    "- Critical for environments with cgroup memory limits (containers, cloud notebooks)\n",
    "- Prevents OOM crashes that would occur with pandas on large datasets\n",
    "\n",
    "**Join Strategy:**\n",
    "- Left join on `TransactionID` preserves all transactions\n",
    "- Identity features are optional enrichment (some transactions lack identity data)\n",
    "- Result is written to Parquet for efficient columnar storage\n",
    "\n",
    "**Caching:**\n",
    "- If the joined Parquet file already exists, we skip this step\n",
    "- Saves time on repeated runs and notebook restarts\n",
    "\n",
    "This approach is production-ready: it handles datasets larger than RAM and produces a durable, compressed artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f952db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "trx_csv = os.path.join(DATA_DIR, \"train_transaction.csv\")\n",
    "id_csv  = os.path.join(DATA_DIR, \"train_identity.csv\")\n",
    "\n",
    "assert os.path.exists(trx_csv), f\"Missing {trx_csv}\"\n",
    "assert os.path.exists(id_csv),  f\"Missing {id_csv}\"\n",
    "\n",
    "print(\"OK:\", trx_csv)\n",
    "print(\"OK:\", id_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "JOINED_PARQ = os.path.join(DATA_DIR, \"train_joined.parquet\")\n",
    "\n",
    "if not os.path.exists(JOINED_PARQ):\n",
    "    print(\"Building:\", JOINED_PARQ)\n",
    "    trx = pl.scan_csv(trx_csv)\n",
    "    idd = pl.scan_csv(id_csv)\n",
    "    trx.join(idd, on=\"TransactionID\", how=\"left\").sink_parquet(JOINED_PARQ)\n",
    "    print(\"Wrote:\", JOINED_PARQ)\n",
    "else:\n",
    "    print(\"Exists:\", JOINED_PARQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cecd5a9",
   "metadata": {},
   "source": [
    "### Dataset Profiling: Shape and Target Distribution\n",
    "\n",
    "Computes high-level statistics about the dataset:\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Row count**: Total number of transactions\n",
    "- **Fraud rate**: Baseline prevalence of fraud (class balance)\n",
    "- **Column count**: Dimensionality of the feature space\n",
    "\n",
    "**Why This Matters:**\n",
    "- Low fraud rates (~3-4%) indicate severe class imbalance\n",
    "- Imbalanced data requires careful metric selection (AUC, precision-recall, alert rate)\n",
    "- High dimensionality suggests need for feature selection or dimensionality reduction\n",
    "\n",
    "**Schema Inspection:**\n",
    "- Shows first 8 columns with data types\n",
    "- Helps identify categorical vs. numerical features\n",
    "- Informs feature engineering strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b26767",
   "metadata": {},
   "source": [
    "## 3) Data understanding (schema, size, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = pl.scan_parquet(JOINED_PARQ)\n",
    "rows = scan.select(pl.len()).collect().item()\n",
    "fraud_rate = scan.select(pl.col(\"isFraud\").mean()).collect().item()\n",
    "\n",
    "print(\"Rows:\", rows)\n",
    "print(\"Fraud rate:\", round(fraud_rate*100, 3), \"%\")\n",
    "\n",
    "schema = pl.read_parquet_schema(JOINED_PARQ)\n",
    "print(\"Columns:\", len(schema))\n",
    "list(schema.items())[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7f2d9",
   "metadata": {},
   "source": [
    "### Duplicate Detection\n",
    "\n",
    "Checks for duplicate `TransactionID` values in the dataset:\n",
    "\n",
    "**Purpose:**\n",
    "- Duplicates can indicate data quality issues or extraction errors\n",
    "- May lead to data leakage if duplicates appear in both train and validation sets\n",
    "- Can artificially inflate model performance metrics\n",
    "\n",
    "**Expected Result:**\n",
    "- Clean datasets should have zero duplicates\n",
    "- Non-zero count requires investigation and potential deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fdd96",
   "metadata": {},
   "source": [
    "## 4) Data quality checks (duplicates, missingness, cardinality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42a4b1",
   "metadata": {},
   "source": [
    "### Missingness Analysis\n",
    "\n",
    "Quantifies missing values across all columns:\n",
    "\n",
    "**Analysis:**\n",
    "- Computes null count and null percentage for each column\n",
    "- Sorts by missingness to identify problematic features\n",
    "- Displays top 20 columns with highest missing rates\n",
    "\n",
    "**Decision Criteria:**\n",
    "- Features with >80% missingness may have limited predictive value\n",
    "- High missingness in categorical features suggests encoding challenges\n",
    "- Missing patterns (MCAR vs. MAR) may be informative signals themselves\n",
    "\n",
    "**Production Implications:**\n",
    "- High-missingness features require imputation strategy or exclusion\n",
    "- Missingness patterns should be monitored for drift in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_count = (\n",
    "    scan.select(pl.col(\"TransactionID\"))\n",
    "        .collect()\n",
    "        .to_series()\n",
    "        .is_duplicated()\n",
    "        .sum()\n",
    ")\n",
    "print(\"Duplicate TransactionID rows:\", int(dup_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4c099",
   "metadata": {},
   "source": [
    "### Cardinality Profiling for Categorical Features\n",
    "\n",
    "Analyzes the unique value counts of string (categorical) columns:\n",
    "\n",
    "**Purpose:**\n",
    "- High-cardinality features (e.g., DeviceInfo) may require grouping or embeddings\n",
    "- Low-cardinality features are candidates for one-hot encoding\n",
    "- Extremely high cardinality may indicate PII or unique identifiers (potential leakage)\n",
    "\n",
    "**Sampling Strategy:**\n",
    "- Uses a 120K-row sample for computational efficiency\n",
    "- Provides approximate cardinality estimates sufficient for feature engineering decisions\n",
    "\n",
    "**Interpretation:**\n",
    "- Features with 1000+ unique values may benefit from frequency encoding\n",
    "- Features with <20 unique values are ideal for categorical encoding\n",
    "- Single-value features (zero variance) should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = scan.select([pl.all().null_count()]).collect().row(0)\n",
    "cols = list(schema.keys())\n",
    "null_df = pd.DataFrame({\"column\": cols, \"null_count\": null_counts})\n",
    "null_df[\"null_pct\"] = null_df[\"null_count\"] / rows * 100\n",
    "null_df = null_df.sort_values(\"null_pct\", ascending=False)\n",
    "display(null_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_N = 120_000\n",
    "candidate_cats = [c for c, dt in schema.items() if dt == pl.Utf8][:50]\n",
    "if candidate_cats:\n",
    "    df_cat = scan.select(candidate_cats).collect().sample(n=min(EDA_N, rows), seed=42).to_pandas()\n",
    "    nunq = df_cat.nunique(dropna=True).sort_values(ascending=False).head(15)\n",
    "    display(nunq.to_frame(\"n_unique (sampled)\"))\n",
    "else:\n",
    "    print(\"No Utf8/object columns detected in schema sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd249b",
   "metadata": {},
   "source": [
    "### EDA Sample Construction\n",
    "\n",
    "Creates a manageable sample for exploratory data analysis:\n",
    "\n",
    "**Selected Features:**\n",
    "- Core transaction attributes: amount, time, product\n",
    "- Card identifiers: card4 (provider), card6 (type)\n",
    "- Identity signals: email domains, device type/info, addresses\n",
    "\n",
    "**Sampling Strategy:**\n",
    "- 100K-row sample balances statistical power with computational speed\n",
    "- Fixed random seed (7) ensures reproducibility\n",
    "- Sample is small enough to visualize without performance issues\n",
    "\n",
    "**Purpose:**\n",
    "- Enables rapid hypothesis testing and pattern discovery\n",
    "- Reduces iteration time during exploratory phase\n",
    "- Sample-based insights are validated on full data during modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c72d9",
   "metadata": {},
   "source": [
    "## 5) EDA on a safe sample (banking-relevant insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19967f",
   "metadata": {},
   "source": [
    "### Temporal and Magnitude Feature Engineering for EDA\n",
    "\n",
    "Derives interpretable features from raw transaction data:\n",
    "\n",
    "**Time-Based Features:**\n",
    "- `TransactionDay`: Converts Unix timestamp to day index (detects weekly patterns)\n",
    "- `TransactionHour`: Extracts hour-of-day (0-23) for circadian fraud patterns\n",
    "\n",
    "**Amount Binning:**\n",
    "- `AmtBin`: Quantile-based discretization into 10 bins\n",
    "- Enables visualization of fraud rate by amount tier\n",
    "- Handles skewed distributions better than uniform bins\n",
    "\n",
    "**Banking Rationale:**\n",
    "- Fraud risk varies by time (off-hours, weekends)\n",
    "- Small vs. large transactions have different fraud profiles\n",
    "- These patterns inform rule-based alerts and model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680af608",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_COLS = [\n",
    "    \"isFraud\",\"TransactionAmt\",\"TransactionDT\",\n",
    "    \"ProductCD\",\"card4\",\"card6\",\n",
    "    \"P_emaildomain\",\"R_emaildomain\",\n",
    "    \"DeviceType\",\"DeviceInfo\",\n",
    "    \"addr1\",\"addr2\"\n",
    "]\n",
    "EDA_COLS = [c for c in EDA_COLS if c in schema]\n",
    "\n",
    "df_eda = scan.select(EDA_COLS).collect().sample(n=min(100_000, rows), seed=7).to_pandas()\n",
    "print(\"EDA sample:\", df_eda.shape)\n",
    "df_eda.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e9153",
   "metadata": {},
   "source": [
    "### Target Variable Distribution Visualization\n",
    "\n",
    "Displays the class balance between fraudulent and legitimate transactions:\n",
    "\n",
    "**Key Insights:**\n",
    "- Visual confirmation of severe class imbalance\n",
    "- Fraud typically represents 3-4% of transactions\n",
    "- Imbalance necessitates careful metric selection:\n",
    "  - **Avoid**: Accuracy (misleading with imbalance)\n",
    "  - **Use**: AUC-ROC, Precision-Recall, Alert Rate metrics\n",
    "\n",
    "**Business Context:**\n",
    "- High imbalance reflects real-world fraud rates\n",
    "- Models must achieve high recall without excessive false positives\n",
    "- Cost-benefit analysis is more relevant than F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad58ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TransactionDT\" in df_eda.columns:\n",
    "    df_eda[\"TransactionDay\"]  = (df_eda[\"TransactionDT\"] // (3600*24)).astype(\"int32\")\n",
    "    df_eda[\"TransactionHour\"] = ((df_eda[\"TransactionDT\"] // 3600) % 24).astype(\"int16\")\n",
    "if \"TransactionAmt\" in df_eda.columns:\n",
    "    df_eda[\"AmtBin\"] = pd.qcut(df_eda[\"TransactionAmt\"].rank(method=\"first\"), q=10, duplicates=\"drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3e8ab",
   "metadata": {},
   "source": [
    "### Circadian Pattern Analysis: Fraud Rate by Hour\n",
    "\n",
    "Analyzes how fraud risk varies throughout the 24-hour cycle:\n",
    "\n",
    "**Expected Patterns:**\n",
    "- Higher fraud during off-hours (late night, early morning) when monitoring is minimal\n",
    "- Lower fraud during business hours when customers are active\n",
    "- Weekend vs. weekday differences may also emerge\n",
    "\n",
    "**Actionable Insights:**\n",
    "- Temporal features are strong fraud signals\n",
    "- Can inform dynamic threshold adjustments\n",
    "- Helps schedule analyst coverage for high-risk periods\n",
    "\n",
    "This type of analysis is standard in fraud operations and directly translates to model features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "df_eda[\"isFraud\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Class distribution (EDA sample)\")\n",
    "plt.xlabel(\"isFraud\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43658764",
   "metadata": {},
   "source": [
    "### Transaction Amount Risk Stratification\n",
    "\n",
    "Examines fraud rate across transaction amount deciles:\n",
    "\n",
    "**Analysis Technique:**\n",
    "- Rank-based quantile binning avoids issues with skewed distributions\n",
    "- Each bin contains ~10% of transactions\n",
    "- Compares fraud rates across amount tiers\n",
    "\n",
    "**Typical Findings:**\n",
    "- Very small amounts (testing stolen cards) may show elevated fraud\n",
    "- Very large amounts (high-value theft) also show elevated fraud\n",
    "- Mid-range amounts often have lower fraud rates\n",
    "\n",
    "**Business Application:**\n",
    "- Informs amount-based risk scoring rules\n",
    "- Helps set dynamic transaction limits\n",
    "- Identifies sweet spots for fraudsters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TransactionHour\" in df_eda.columns:\n",
    "    grp = df_eda.groupby(\"TransactionHour\")[\"isFraud\"].mean()\n",
    "    plt.figure()\n",
    "    grp.plot(kind=\"line\", marker=\"o\")\n",
    "    plt.title(\"Fraud rate by hour-of-day (EDA sample)\")\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(\"Fraud rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4253b8",
   "metadata": {},
   "source": [
    "### Device Type Fraud Risk Profiling\n",
    "\n",
    "Analyzes fraud rate by device type (mobile, desktop, etc.):\n",
    "\n",
    "**Banking Context:**\n",
    "- Mobile devices may have different fraud profiles than desktops\n",
    "- Unknown/missing device types are often high-risk\n",
    "- Helps identify compromised device populations\n",
    "\n",
    "**Visualization Strategy:**\n",
    "- Focuses on top 10 most common device types\n",
    "- Rare devices aggregated to avoid noise\n",
    "- Sorted by fraud rate to highlight highest-risk segments\n",
    "\n",
    "**Operational Use:**\n",
    "- High-risk device types can trigger additional authentication\n",
    "- Device fingerprinting becomes a key fraud signal\n",
    "- Informs device-based blocking rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"AmtBin\" in df_eda.columns:\n",
    "    grp = df_eda.groupby(\"AmtBin\")[\"isFraud\"].mean()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    grp.plot(kind=\"bar\")\n",
    "    plt.title(\"Fraud rate by TransactionAmt decile (EDA sample)\")\n",
    "    plt.xlabel(\"Amount decile (rank-based)\")\n",
    "    plt.ylabel(\"Fraud rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8497b3",
   "metadata": {},
   "source": [
    "### Email Domain Risk Analysis\n",
    "\n",
    "Examines fraud rates by purchaser (`P_emaildomain`) and recipient (`R_emaildomain`) email domains:\n",
    "\n",
    "**Key Patterns:**\n",
    "- Free email providers (gmail, yahoo) vs. corporate domains\n",
    "- Disposable/temporary email services show elevated fraud\n",
    "- Missing email domains are often high-risk\n",
    "\n",
    "**Banking Intelligence:**\n",
    "- Email domain is a strong identity verification signal\n",
    "- Mismatch between purchaser and recipient domains can indicate fraud\n",
    "- Domain reputation lists inform real-time decisioning\n",
    "\n",
    "**Visualization:**\n",
    "- Top 12 most common domains to focus on significant traffic\n",
    "- Rare domains aggregated to reduce noise\n",
    "- Separate charts for purchaser and recipient for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4393d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DeviceType\" in df_eda.columns:\n",
    "    top = df_eda[\"DeviceType\"].value_counts(dropna=False).head(10).index\n",
    "    sub = df_eda[df_eda[\"DeviceType\"].isin(top)]\n",
    "    grp = sub.groupby(\"DeviceType\")[\"isFraud\"].mean().sort_values(ascending=False)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    grp.plot(kind=\"bar\")\n",
    "    plt.title(\"Fraud rate by DeviceType (top 10, EDA sample)\")\n",
    "    plt.xlabel(\"DeviceType\")\n",
    "    plt.ylabel(\"Fraud rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbd50b",
   "metadata": {},
   "source": [
    "### Comprehensive Missingness Profiling\n",
    "\n",
    "Provides dual view of missing data patterns:\n",
    "\n",
    "**Table View:**\n",
    "- Top 20 columns by missingness percentage\n",
    "- Identifies features requiring imputation or exclusion\n",
    "- Flags columns with near-complete missingness\n",
    "\n",
    "**Distribution View:**\n",
    "- Histogram of missingness rates across all columns\n",
    "- Shows whether missingness is concentrated or widespread\n",
    "- Helps assess overall data quality\n",
    "\n",
    "**Strategic Decisions:**\n",
    "- Features with >80% missing: typically excluded\n",
    "- Features with 20-80% missing: require sophisticated imputation\n",
    "- Features with <20% missing: simple imputation (median, mode) often sufficient\n",
    "\n",
    "Missing data handling is critical for model robustness and production reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"P_emaildomain\",\"R_emaildomain\"]:\n",
    "    if col in df_eda.columns:\n",
    "        top = df_eda[col].value_counts(dropna=False).head(12).index\n",
    "        sub = df_eda[df_eda[col].isin(top)]\n",
    "        grp = sub.groupby(col)[\"isFraud\"].mean().sort_values(ascending=False)\n",
    "        plt.figure(figsize=(10,4))\n",
    "        grp.plot(kind=\"bar\")\n",
    "        plt.title(f\"Fraud rate by {col} (top 12, EDA sample)\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Fraud rate\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss = (df_eda.isna().mean()*100).sort_values(ascending=False).head(20)\n",
    "display(miss.to_frame(\"missing_% (EDA sample)\"))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(df_eda.isna().mean(axis=0).values, bins=30)\n",
    "plt.title(\"Missingness distribution across columns (EDA sample)\")\n",
    "plt.xlabel(\"Fraction missing\")\n",
    "plt.ylabel(\"# columns\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154989e",
   "metadata": {},
   "source": [
    "### Production-Grade Feature Engineering Pipeline\n",
    "\n",
    "Transforms raw data into model-ready numeric features using Polars streaming:\n",
    "\n",
    "**Feature Selection:**\n",
    "- Keeps **V columns** (V1-V250): Vesta-engineered features (industry-standard fraud signals)\n",
    "- Keeps **categorical columns**: ProductCD, card identifiers, email domains, device info, addresses\n",
    "\n",
    "**Feature Engineering Operations:**\n",
    "\n",
    "1. **Frequency Encoding** (for categorical features):\n",
    "   - Counts occurrences of each category value\n",
    "   - High-frequency values often indicate legitimate patterns\n",
    "   - Low-frequency values can signal novel fraud attempts\n",
    "\n",
    "2. **Amount Transformation**:\n",
    "   - `TransactionAmt_log1p`: Log transform to handle skewness and outliers\n",
    "   - Improves model convergence and reduces sensitivity to extreme values\n",
    "\n",
    "3. **Temporal Features**:\n",
    "   - `TransactionDay`: Day index for weekly patterns\n",
    "   - `TransactionHour`: Hour-of-day for circadian patterns\n",
    "\n",
    "4. **Interaction Features**:\n",
    "   - `Amt_x_Hour`: Amount-time interaction (large late-night transactions are risky)\n",
    "   - `Amt_div_Day`: Amount velocity proxy (spending rate over time)\n",
    "\n",
    "**Pipeline Characteristics:**\n",
    "- **Streaming**: Processes data in chunks to avoid OOM\n",
    "- **Type Safety**: Casts all features to Float32 for memory efficiency\n",
    "- **Caching**: Writes to Parquet for reuse across experiments\n",
    "- **Reproducibility**: Deterministic transformations, no randomness\n",
    "\n",
    "This pipeline is production-ready: it handles large datasets, produces consistent outputs, and can be deployed as a batch or real-time scoring pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de068a",
   "metadata": {},
   "source": [
    "## 6) Feature engineering pipeline (streaming) → compact numeric feature Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c18c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_PARQ = os.path.join(DATA_DIR, \"train_features.parquet\")\n",
    "\n",
    "V_MAX = 250\n",
    "CAT_CANDS = [\"ProductCD\",\"card4\",\"card6\",\"P_emaildomain\",\"R_emaildomain\",\"DeviceType\",\"DeviceInfo\",\"addr1\",\"addr2\"]\n",
    "\n",
    "schema = pl.read_parquet_schema(JOINED_PARQ)\n",
    "v_cols = [f\"V{i}\" for i in range(1, V_MAX+1) if f\"V{i}\" in schema]\n",
    "cat_cols = [c for c in CAT_CANDS if c in schema]\n",
    "\n",
    "base_cols = [\"TransactionID\",\"isFraud\",\"TransactionDT\",\"TransactionAmt\"]\n",
    "keep_cols = base_cols + v_cols + cat_cols\n",
    "\n",
    "print(\"Keeping V cols:\", len(v_cols))\n",
    "print(\"Freq-encoding cats:\", cat_cols)\n",
    "\n",
    "if not os.path.exists(FEAT_PARQ):\n",
    "    base = pl.scan_parquet(JOINED_PARQ).select(keep_cols)\n",
    "    feat = base\n",
    "\n",
    "    for c in cat_cols:\n",
    "        ft = base.group_by(c).agg(pl.len().alias(f\"{c}_freq\"))\n",
    "        feat = feat.join(ft, on=c, how=\"left\")\n",
    "\n",
    "    feat = feat.with_columns([\n",
    "        pl.col(\"TransactionAmt\").clip(0).log1p().alias(\"TransactionAmt_log1p\"),\n",
    "        (pl.col(\"TransactionDT\") // (3600*24)).cast(pl.Int32).alias(\"TransactionDay\"),\n",
    "        ((pl.col(\"TransactionDT\") // 3600) % 24).cast(pl.Int16).alias(\"TransactionHour\"),\n",
    "        (pl.col(\"TransactionAmt\").fill_null(0) * ((pl.col(\"TransactionDT\") // 3600) % 24).cast(pl.Float32)).alias(\"Amt_x_Hour\"),\n",
    "        (pl.col(\"TransactionAmt\").fill_null(0) / ((pl.col(\"TransactionDT\") // (3600*24)).cast(pl.Float32) + 1.0)).alias(\"Amt_div_Day\"),\n",
    "    ])\n",
    "\n",
    "    feat = feat.drop(cat_cols)\n",
    "    feat = feat.with_columns([pl.all().exclude([\"TransactionID\",\"isFraud\"]).cast(pl.Float32, strict=False)])\n",
    "    feat.sink_parquet(FEAT_PARQ)\n",
    "    print(\"Wrote:\", FEAT_PARQ)\n",
    "else:\n",
    "    print(\"Exists:\", FEAT_PARQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fcb12e",
   "metadata": {},
   "source": [
    "### Baseline Model Training and Evaluation\n",
    "\n",
    "Trains a fast baseline classifier to establish performance benchmarks:\n",
    "\n",
    "**Data Preparation:**\n",
    "- Loads engineered features from Parquet\n",
    "- Separates target (`isFraud`) from feature matrix\n",
    "- Handles infinite values and missing data (fill with sentinel value -999)\n",
    "- Converts to Float32 for memory efficiency\n",
    "\n",
    "**Train-Validation Split:**\n",
    "- 80/20 split with stratification to preserve class balance\n",
    "- Fixed random seed (42) for reproducibility\n",
    "- Ensures both sets have similar fraud rates\n",
    "\n",
    "**Model Selection:**\n",
    "- **SGDClassifier** with log loss (logistic regression trained via SGD)\n",
    "- Fast to train, interpretable, serves as sanity check\n",
    "- Regularization (alpha=1e-5) prevents overfitting\n",
    "\n",
    "**Feature Scaling:**\n",
    "- StandardScaler (zero mean, unit variance)\n",
    "- Critical for SGD convergence\n",
    "- Fit on training data, applied to validation (no leakage)\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- **ROC-AUC**: Overall discriminative ability (threshold-agnostic)\n",
    "- **PR-AUC**: Precision-recall trade-off (better for imbalanced data)\n",
    "- Training time benchmarked for comparison\n",
    "\n",
    "This baseline establishes the minimum performance bar. More complex models must beat this to justify their cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3b2aa",
   "metadata": {},
   "source": [
    "## 7) Baseline model + Decisioning (alert-rate & cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634c691",
   "metadata": {},
   "source": [
    "### Alert-Rate Decisioning\n",
    "\n",
    "Applies an **operational decisioning strategy** based on alert rate:\n",
    "\n",
    "**Business Context:**\n",
    "- Banks have finite capacity to review alerts (analyst hours, customer friction)\n",
    "- Alert rate = percentage of transactions flagged for review\n",
    "- Common targets: 0.5%, 1%, 2% depending on review capacity\n",
    "\n",
    "**Threshold Selection:**\n",
    "- Sets threshold to flag top 1% of riskiest transactions\n",
    "- Equivalent to 99th percentile of model scores\n",
    "\n",
    "**Performance Evaluation:**\n",
    "- **Confusion Matrix**: Shows TP, FP, TN, FN at this operating point\n",
    "- **Precision**: What % of alerts are actual fraud?\n",
    "- **Recall**: What % of fraud is caught?\n",
    "\n",
    "**Operational Interpretation:**\n",
    "- High precision = efficient use of analyst time\n",
    "- High recall = effective fraud prevention\n",
    "- Trade-off is managed via alert rate constraint\n",
    "\n",
    "This approach is how fraud models are actually deployed: not as classifiers, but as ranking systems with operational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4344977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
    "\n",
    "df_feat = pd.read_parquet(FEAT_PARQ)\n",
    "print(\"Feature df:\", df_feat.shape)\n",
    "\n",
    "y = df_feat[\"isFraud\"].astype(np.int64).values\n",
    "X = (df_feat.drop(columns=[\"isFraud\",\"TransactionID\"])\n",
    "     .replace([np.inf,-np.inf], np.nan)\n",
    "     .fillna(-999)\n",
    "     .astype(np.float32)\n",
    "     .values)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_valid = scaler.transform(X_valid).astype(np.float32)\n",
    "\n",
    "baseline = SGDClassifier(loss=\"log_loss\", alpha=1e-5, max_iter=2000, tol=1e-3, random_state=42)\n",
    "\n",
    "t0 = time.time()\n",
    "baseline.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "scores = baseline.predict_proba(X_valid)[:, 1]\n",
    "print(\"Baseline time (s):\", round(t1-t0, 2))\n",
    "print(\"Baseline ROC-AUC :\", round(roc_auc_score(y_valid, scores), 4))\n",
    "print(\"Baseline PR-AUC  :\", round(average_precision_score(y_valid, scores), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962eee6",
   "metadata": {},
   "source": [
    "### Cost-Based Threshold Optimization\n",
    "\n",
    "Optimizes decision threshold based on **economic costs** rather than statistical metrics:\n",
    "\n",
    "**Cost Model:**\n",
    "- **False Negative (missed fraud)**: $100 per incident (fraud loss + chargeback)\n",
    "- **False Positive (false alarm)**: $1 per incident (review cost + customer friction)\n",
    "- These ratios are business-specific and should be calibrated to actual operations\n",
    "\n",
    "**Optimization Process:**\n",
    "- Sweeps 200 candidate thresholds from 0.001 to 0.999\n",
    "- Computes expected cost at each threshold\n",
    "- Selects threshold minimizing total cost\n",
    "\n",
    "**Cost Function:**\n",
    "```\n",
    "Expected Cost = (FN × $100) + (FP × $1)\n",
    "```\n",
    "\n",
    "**Results:**\n",
    "- Optimal threshold balances fraud losses against review costs\n",
    "- Confusion matrix at optimal point\n",
    "- Cost curve visualization shows sensitivity to threshold choice\n",
    "\n",
    "**Business Value:**\n",
    "- Directly ties model decisions to P&L impact\n",
    "- Supports ROI calculations for fraud prevention programs\n",
    "- Enables what-if analysis (e.g., \"What if review costs double?\")\n",
    "\n",
    "This cost-based approach aligns ML directly with business objectives, making it easier to justify investments and operational changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ca089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alert-rate threshold (top 1% flagged)\n",
    "alert_rate = 0.01\n",
    "thr_alert = float(np.quantile(scores, 1 - alert_rate))\n",
    "y_hat_alert = (scores >= thr_alert).astype(int)\n",
    "\n",
    "print(\"Alert rate:\", alert_rate, \"Threshold:\", thr_alert)\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_valid, y_hat_alert))\n",
    "print(classification_report(y_valid, y_hat_alert, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-based threshold\n",
    "COST_FN = 100.0\n",
    "COST_FP = 1.0\n",
    "\n",
    "def expected_cost(y_true, y_pred, cost_fn, cost_fp):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return fn*cost_fn + fp*cost_fp\n",
    "\n",
    "thresholds = np.linspace(0.001, 0.999, 200)\n",
    "costs = []\n",
    "for thr in thresholds:\n",
    "    y_hat = (scores >= thr).astype(int)\n",
    "    costs.append(expected_cost(y_valid, y_hat, COST_FN, COST_FP))\n",
    "\n",
    "best_i = int(np.argmin(costs))\n",
    "thr_cost = float(thresholds[best_i])\n",
    "print(\"Best threshold (cost-based):\", thr_cost, \"Expected cost:\", round(costs[best_i],2))\n",
    "\n",
    "y_hat_cost = (scores >= thr_cost).astype(int)\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_valid, y_hat_cost))\n",
    "print(classification_report(y_valid, y_hat_cost, digits=4))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, costs)\n",
    "plt.title(\"Expected cost vs threshold (baseline)\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Expected cost\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3f64a",
   "metadata": {},
   "source": [
    "### GPU Acceleration Setup and Data Loading\n",
    "\n",
    "Prepares PyTorch infrastructure for CPU vs. GPU benchmarking:\n",
    "\n",
    "**Environment Check:**\n",
    "- Validates PyTorch installation and version\n",
    "- Detects CUDA availability (GPU acceleration)\n",
    "- Identifies specific GPU model if available\n",
    "\n",
    "**Data Preparation:**\n",
    "- Converts NumPy arrays to PyTorch tensors\n",
    "- Creates `TensorDataset` for efficient data access\n",
    "- Wraps in `DataLoader` for batch iteration\n",
    "\n",
    "**DataLoader Configuration:**\n",
    "- **Batch size**: 8192 (large batches maximize GPU utilization)\n",
    "- **num_workers**: 0 (avoids multiprocessing issues with limited shm)\n",
    "- **pin_memory**: False (disabled for safety in containerized environments)\n",
    "- **shuffle**: True for training, False for validation\n",
    "\n",
    "**Why These Settings?**\n",
    "- Large batches amortize GPU kernel launch overhead\n",
    "- Single-process loading avoids shm exhaustion\n",
    "- No pinned memory prevents CUDA OOM in memory-constrained environments\n",
    "\n",
    "This configuration is optimized for the detected environment (containerized, possibly GPU-accelerated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c2984",
   "metadata": {},
   "source": [
    "## 8) CPU vs GPU Benchmark (PyTorch MLP) — epochs=2 (shm-safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275eb5ca",
   "metadata": {},
   "source": [
    "### Neural Network Architecture Definition\n",
    "\n",
    "Defines a Multi-Layer Perceptron (MLP) for fraud classification:\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input → [4096 ReLU Dropout(0.1)] → [2048 ReLU Dropout(0.1)] → [1024 ReLU] → [2 logits]\n",
    "```\n",
    "\n",
    "**Design Rationale:**\n",
    "- **Large layers**: 4096 and 2048 neurons leverage GPU parallel compute\n",
    "- **ReLU activation**: Fast, gradient-friendly, GPU-optimized\n",
    "- **Dropout**: 10% regularization prevents overfitting on imbalanced data\n",
    "- **Binary output**: 2 logits for CrossEntropyLoss (fraud vs. legitimate)\n",
    "\n",
    "**GPU Suitability:**\n",
    "- Large matrix multiplications (4096×2048) saturate GPU cores\n",
    "- Batch processing (8192 samples) maximizes throughput\n",
    "- Simple operations (ReLU, Dropout) have efficient CUDA kernels\n",
    "\n",
    "**Why Not Tree-Based Models for Benchmarking?**\n",
    "- Trees (XGBoost, LightGBM) are serial by nature\n",
    "- Neural networks exhibit massive parallelism, ideal for GPU comparison\n",
    "- This architecture demonstrates GPU advantage at scale\n",
    "\n",
    "This model is deliberately overparameterized for benchmarking purposes—it showcases GPU compute advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0405c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "valid_ds = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n",
    "\n",
    "BATCH_SIZE = 8192\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(\"DataLoader: batch_size=\", BATCH_SIZE, \"num_workers=0 pin_memory=False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162deba",
   "metadata": {},
   "source": [
    "### CPU vs. GPU Training Benchmark\n",
    "\n",
    "Executes controlled performance comparison between CPU and GPU training:\n",
    "\n",
    "**Benchmark Design:**\n",
    "\n",
    "1. **Training Function** (`train_eval`):\n",
    "   - Accepts device ('cpu' or 'cuda'), epochs, learning rate\n",
    "   - Uses AdamW optimizer (standard for deep learning)\n",
    "   - CrossEntropyLoss for binary classification\n",
    "   - Mixed precision (AMP) for GPU (FP16 compute, FP32 accumulation)\n",
    "\n",
    "2. **Timing Methodology**:\n",
    "   - **Warmup pass**: Excludes cold-start overhead (kernel compilation, cache warming)\n",
    "   - **Synchronized timing**: `torch.cuda.synchronize()` ensures accurate GPU measurements\n",
    "   - Only training time measured (excludes data loading and setup)\n",
    "\n",
    "3. **Controlled Variables**:\n",
    "   - Same model architecture\n",
    "   - Same number of epochs (2 — short to keep demo fast)\n",
    "   - Same batch size and learning rate\n",
    "   - Same data and random seed\n",
    "\n",
    "4. **Evaluation**:\n",
    "   - Computes ROC-AUC on validation set after training\n",
    "   - Ensures both CPU and GPU models achieve similar quality\n",
    "\n",
    "**Expected Results:**\n",
    "- CPU: Slower but always available\n",
    "- GPU: 5-20x faster depending on hardware (A100, V100, T4, etc.)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Faster iteration = more experiments = better models\n",
    "- Batch scoring throughput for real-time fraud detection\n",
    "- ROI justification for GPU infrastructure\n",
    "\n",
    "**GPU Fallback:**\n",
    "- If no CUDA detected, GPU benchmark is skipped gracefully\n",
    "- CPU-only mode still produces valid results\n",
    "\n",
    "This benchmark demonstrates tangible business value of GPU acceleration: time-to-market and throughput gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, 4096), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(4096, 2048), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(2048, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 2),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(device=\"cpu\", epochs=2, lr=1e-3, amp=True):\n",
    "    dev = torch.device(device)\n",
    "    model = MLP(X_train.shape[1]).to(dev)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    use_amp = (amp and dev.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "    # warmup\n",
    "    model.train()\n",
    "    xb, yb = next(iter(train_loader))\n",
    "    xb, yb = xb.to(dev), yb.to(dev)\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "        loss = crit(model(xb), yb)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(opt); scaler.update()\n",
    "\n",
    "    if dev.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(dev), yb.to(dev)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "    if dev.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    ps, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in valid_loader:\n",
    "            xb = xb.to(dev)\n",
    "            prob = torch.softmax(model(xb), dim=1)[:, 1].cpu().numpy()\n",
    "            ps.append(prob); ys.append(yb.numpy())\n",
    "    p = np.concatenate(ps); yt = np.concatenate(ys)\n",
    "    auc = roc_auc_score(yt, p)\n",
    "\n",
    "    return (t1 - t0), auc\n",
    "\n",
    "cpu_t, cpu_auc = train_eval(device=\"cpu\", epochs=2, amp=False)\n",
    "print(\"CPU  time:\", round(cpu_t, 2), \"AUC:\", round(cpu_auc, 4))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_t, gpu_auc = train_eval(device=\"cuda\", epochs=2, amp=True)\n",
    "    print(\"GPU  time:\", round(gpu_t, 2), \"AUC:\", round(gpu_auc, 4))\n",
    "    print(\"Speedup:\", round(cpu_t/gpu_t, 2), \"x\")\n",
    "else:\n",
    "    gpu_t, gpu_auc = None, None\n",
    "    print(\"CUDA not available; GPU benchmark skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef3639",
   "metadata": {},
   "source": [
    "### Summary Dashboard\n",
    "\n",
    "**Summary Table:**\n",
    "- Dataset size and split (train/validation rows)\n",
    "- Feature dimensionality (number of numeric features)\n",
    "- Class balance (fraud rate)\n",
    "- Baseline model performance (ROC-AUC, PR-AUC)\n",
    "- Neural model benchmarks:\n",
    "  - CPU training time and AUC\n",
    "  - GPU training time and AUC (if available)\n",
    "  - Speedup factor (CPU time / GPU time)\n",
    "\n",
    "**Operational Metrics Table:**\n",
    "- **Alert-rate threshold** (top 1% flagged):\n",
    "  - Threshold value\n",
    "  - Precision, recall, alert rate\n",
    "  - Confusion matrix (TP, FP, TN, FN)\n",
    "  \n",
    "- **Cost-based threshold** (minimizing expected cost):\n",
    "  - Optimal threshold value\n",
    "  - Precision, recall, alert rate\n",
    "  - Confusion matrix\n",
    "  - Expected cost at optimal operating point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ee030",
   "metadata": {},
   "source": [
    "## 9) Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_auc = roc_auc_score(y_valid, scores)\n",
    "baseline_prauc = average_precision_score(y_valid, scores)\n",
    "\n",
    "def summarize_threshold(name, thr, y_true, scores):\n",
    "    y_hat = (scores >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
    "    return {\n",
    "        \"threshold_name\": name,\n",
    "        \"threshold\": float(thr),\n",
    "        \"TP\": int(tp), \"FP\": int(fp), \"FN\": int(fn), \"TN\": int(tn),\n",
    "        \"precision\": float(tp/(tp+fp+1e-9)),\n",
    "        \"recall\": float(tp/(tp+fn+1e-9)),\n",
    "        \"alert_rate_empirical\": float(y_hat.mean()),\n",
    "    }\n",
    "\n",
    "ops = pd.DataFrame([\n",
    "    summarize_threshold(\"alert_rate_1pct\", thr_alert, y_valid, scores),\n",
    "    summarize_threshold(\"cost_based\", thr_cost, y_valid, scores),\n",
    "])\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"rows_total\": int(rows),\n",
    "    \"rows_train\": int(X_train.shape[0]),\n",
    "    \"rows_valid\": int(X_valid.shape[0]),\n",
    "    \"num_features\": int(X_train.shape[1]),\n",
    "    \"fraud_rate_%\": float(fraud_rate*100),\n",
    "    \"baseline_ROC_AUC\": float(baseline_auc),\n",
    "    \"baseline_PR_AUC\": float(baseline_prauc),\n",
    "    \"mlp_epochs\": 2,\n",
    "    \"cpu_time_sec\": float(cpu_t),\n",
    "    \"cpu_auc\": float(cpu_auc),\n",
    "    \"gpu_time_sec\": (float(gpu_t) if gpu_t is not None else None),\n",
    "    \"gpu_auc\": (float(gpu_auc) if gpu_auc is not None else None),\n",
    "    \"speedup_cpu_over_gpu\": (float(cpu_t/gpu_t) if gpu_t is not None else None),\n",
    "}])\n",
    "\n",
    "display(summary)\n",
    "display(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87847d2a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (conda)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
